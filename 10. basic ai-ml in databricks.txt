# Databricks Notebook Cell: Basic Data Preparation and Feature Engineering

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, isnan
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml import Pipeline

spark = SparkSession.builder.getOrCreate()

# Sample dataset: Customer info with some missing values
data = [
    (1, "Male", 25, 50000, "Yes"),
    (2, "Female", 30, None, "No"),
    (3, "Female", 22, 40000, "Yes"),
    (4, "Male", None, 45000, "No"),
    (5, "Male", 35, 80000, "Yes"),
    (6, "Female", 40, 60000, None),
    (7, "Male", 28, 52000, "No"),
    (8, None, 23, 48000, "Yes"),
    (9, "Female", 27, 62000, "No"),
    (10, "Male", 31, None, "Yes"),
]

columns = ["id", "gender", "age", "income", "purchased"]

df = spark.createDataFrame(data, schema=columns)

# Handle missing values: Fill missing income with median
median_income = df.approxQuantile("income", [0.5], 0.0)[0]
df = df.na.fill({"income": median_income})

# Fill missing age with mean
mean_age = df.selectExpr("avg(age)").collect()[0][0]
df = df.na.fill({"age": mean_age})

# Fill missing gender with 'Unknown'
df = df.na.fill({"gender": "Unknown"})

# Fill missing purchased with 'No'
df = df.na.fill({"purchased": "No"})

# Encode categorical features
gender_indexer = StringIndexer(inputCol="gender", outputCol="gender_index")
purchased_indexer = StringIndexer(inputCol="purchased", outputCol="label")

# Feature assembler
assembler = VectorAssembler(inputCols=["gender_index", "age", "income"], outputCol="features")

pipeline = Pipeline(stages=[gender_indexer, purchased_indexer, assembler])
model = pipeline.fit(df)
df_prepped = model.transform(df).select("features", "label")

df_prepped.show(truncate=False)


# 2. Train/Test Split and Simple Logistic Regression Model

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Split data
train_df, test_df = df_prepped.randomSplit([0.7, 0.3], seed=42)

# Train logistic regression model
lr = LogisticRegression(featuresCol="features", labelCol="label")
lr_model = lr.fit(train_df)

# Predictions
predictions = lr_model.transform(test_df)
predictions.select("features", "label", "prediction", "probability").show(truncate=False)

# Evaluate model
evaluator = BinaryClassificationEvaluator()
auc = evaluator.evaluate(predictions)
print(f"Test AUC = {auc:.3f}")


# 3. Intermediate: Random Forest Classification

from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10)
rf_model = rf.fit(train_df)

rf_predictions = rf_model.transform(test_df)
rf_predictions.select("features", "label", "prediction", "probability").show(truncate=False)

rf_auc = evaluator.evaluate(rf_predictions)
print(f"Random Forest Test AUC = {rf_auc:.3f}")


# 4. Python: Simple ML with Scikit-Learn (Run in Databricks Python cell)

# Basic Scikit-Learn regression example

from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import numpy as np

# Generate synthetic regression data
X, y = make_regression(n_samples=200, n_features=3, noise=0.3, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Linear Regression MSE: {mse:.3f}")


# 5. Python: Basic Neural Network with Keras (Run in Databricks Python cell)